## 用户活跃度
交互式用户行为分析就是要在页面快速得出分析结果，与离线分析相对。
Spark RDD API来开发这些复杂的业务逻辑，为什么不直接用SQL？
用SQL是可以的，但是要区分一下，SQL主要适用于大量离线批处理的ETL作业和统计分析逻辑，
统计分析报表需求灵活，多变，经常会增加，经常逻辑会变，用SQL是很合适的。

但是我们这个是一套系统，和java web配合的系统，模块和需求都是固定的。
用SQL的缺点在于，Spark底层自动生成执行计划和代码，我们几乎无法进行深度的调优，遇到问题也不好解决。
但是对于我们这种固定需求，少量模块，要求速度和稳定性的系统来说，使用Spark RDD API是最好的选择，因为RDD是最原始的API，我们几乎可以控制一切。
包括参数调优以及数据倾斜的重构和优化等等，遇到报错，都是最底层的源码，我们可以很容易进行定位和修复问题。
这就是为什么本套系统大量采用Spark RDD API开发复杂业务模块的原因。

但是现在spark2.x中 spark sql的各种内嵌的性能优化是比人裸写RDD遵守各种所谓的最佳实践更靠谱的，尤其对新手来讲。
比如有些最佳实践讲到先filter操作再 map 操作，这种spark sql中会自动进行谓词下推，比如尽量避免使用 shuffle 操作。
spark sql 中如果你开启了相关的配置，会自动使用 broadcast join 来广播小表，把 shuffle join 转化为 map join 等等，真的能让我们省很多心。

用户活跃度分析

1、指定时间段内访问次数最多的10个用户
2、指定时间段内购买商品金额最多的10个用户
3、最近周期内相对之前一个周期访问次数增长最快的10个用户
4、最近周期内相对之前一个周期购买商品金额增长最快的10个用户
5、指定周期内注册的新用户在头7天访问次数最多的10个用户
