## 各区域热门商品统计

### 需求
根据用户指定的日期范围，统计各个区域下的最热门的top3商品（topn排序）

### 表设计
基础数据的准备和设计
1. MySQL表city_info，字段city_info，city_id、city_name、area
2. Hive表product_info，字段product_id、product_name、info (目前也放在mysql)
3. MySQL结果表area_top3_product，字段task_id、area、area_level、product_id、city_names、click_count、product_name、product_status

### 技术方案设计
1. 区域信息在哪里，各个城市的信息，城市是不怎么变化的，没有必要存储在hive里？MySQL，Hive和MySQL异构数据源使用
2. hive用户行为数据，和mysql城市信息，join，关联之后是RDD？RDD转换DataFrame，注册临时表
3. 各个区域下各个商品的点击量，保留每个区域的城市列表数据？自定义UDAF函数，group_concat_distinct()
4. product_id，join hive表中的商品信息，商品信息在哪里？Hive。商品的经营类型是什么？自定义UDF函数，get_json_object()，内置函数if()做判断
5. 获取每个区域的点击量top3商品？使用开窗函数；使用case when给每个区域打上级别的标识，北京、上海，发达，标记A类
6. Spark SQL的数据倾斜解决方案？双重group by、随机key以及扩容表（自定义UDF函数，random_key()）、内置reduce join转换为map join、shuffle并行度

### 编码流程
1. 从task获取日期范围，查询sparksql中user_visit_action表中的指定日期范围内的数据，过滤出，商品点击行为。
2. 使用Spark SQL从MySQL中查询出城市信息，用户访问行为数据要跟城市信息进行join，得到RDD(city_id、city_name、area、product_id)，转换成DataFrame注册成一个临时表。
3. 计算出来每个区域下每个商品的点击次数，group by area, product_id；保留每个区域的城市名称列表（已经根据区域group了，怎么还能保留城市名呢？用udaf就行）；自定义UDAF，group_concat_distinct()函数，聚合出来一个city_names字段，area、product_id、city_names、click_count
4. join商品明细hive表（product_id、product_name、info），info是json类型，自定义UDF，get_json_object()函数，取出其中的product_status字段，if()函数（Spark SQL内置函数），判断，0 自营，1 第三方；（area、product_id、city_names、click_count、product_name、product_status）
5. 开窗函数，根据area来聚合，获取每个area下，click_count排名前3的product信息；area、area_level、product_id、city_names、click_count、product_name、product_status
6. 使用Spark SQL内置函数（case when），对area打标记（华东大区，A级，华中大区，B级，东北大区，C级，西北大区，D级）得到area_level。又要再注册一个表？ 不用啊！就是在第5的sql里面做就行了。
7. 结果写入MySQL表中
8. Spark SQL的数据倾斜解决方案：双重group by、随机key以及扩容表（自定义UDF函数，random_key()）、Spark SQL内置的reduce join转换为map join、提高shuffle并行度
9. 本地测试和生产环境的测试。

### 优化
1. 聚合源数据：Spark Core和Spark SQL没有任何的区别
2. 过滤导致倾斜的key：在sql中用where条件
3. 提高shuffle并行度：groupByKey(1000)，spark.sql.shuffle.partitions（默认是200）
4. 双重group by：改写SQL，两次group by
5. reduce join转换为map join：spark.sql.autoBroadcastJoinThreshold（默认是10485760）
  你可以自己将表做成RDD，自己手动去实现map join
  Spark SQL内置的map join，默认是如果有一个小表，是在10M以内，默认就会将该表进行broadcast，然后执行map join；调节这个阈值，比如调节到20M. 50M. 甚至1G。
6. 采样倾斜key并单独进行join：纯Spark Core的一种方式，sample. filter等算子
7. 随机key与扩容表：Spark SQL+Spark Core

由于Spark的这种都是基于RDD的特性；用纯RDD，也能够实现一模一样的功能。
之前使用在Spark Core中的数据倾斜解决方案，全部都可以直接套用在Spark SQL上。


### 收获
1. 业务中如何使用SparkSql
2. udf和udaf的使用（UDF：一进一出  UDAF:多进一出）
    * UDF，即最基本的自定义函数，类似to_char,to_date等。平时有什么sql不能弄的就用这自定义一下很方便。例如判断一下某个字段是否为空，像这个需求里面就用来做字符串的连接。
    * UDAF（User-Defined Aggregation Funcation），用户自定义聚合函数，类似在group by之前使用的sum,avg等。
    * UDTF(User-Defined Table-Generating Functions),用户自定义生成函数，有点像stream里面的flatMap
3. 重点要学会在实际的业务中应用学到的知识。关于udaf，真正的业务场景里面，总会有千奇百怪的需求，比如：
    * 想要按照某个字段分组，取其中的一个最大值
    * 想要按照某个字段分组，对分组内容的数据按照特定字段统计累加
    * 想要按照某个字段分组，要符合特定的条件，最后拼接字符串，如果不用UDAF，你要是写spark可能需要这样做：
    ```java
    rdd.groupBy(r->r.xxx)
        .map(t2->{
            HashSet<String> set = new HashSet<>();
            for(Object p : t2._2){
                if(p.getBs() > 0 ){
                    map.put(xx,yyy)
                }
            }
            return StringUtils.join(set.toArray(),",");
        });
    //这样写，其实也能应付需求了，但是代码显得略有点丑陋。还是不如SparkSQL看的清晰明了...
    ```

