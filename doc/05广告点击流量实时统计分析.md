## 广告点击流量实时统计分析

### 背景
网站app会有一些第三方的客户打一些广告；也是一些互联网公司的核心收入来源；
广告被用户点击以后，我们要针对这种用户行为（广告点击行为）实时数据，进行实时的大数据计算和统计。
每次点击一个广告以后，网站app中都会有埋点往后台发送一条日志数据；要做实时统计的话，会将数据写入到分布式消息队列中。
我们要负责编写实时计算程序，去从消息队列中（kafka）去实时地拉取数据，然后对数据进行实时的计算和统计。

这个模块的意义在于，让产品经理. 高管可以实时地掌握到公司打的各种广告的投放效果。
以便于后期持续地对公司的广告投放相关的战略和策略，进行调整和优化；以期望获得最好的广告收益。

### 需求分析：
1.  实现实时的动态黑名单机制：将每天对某个广告点击超过100次的用户拉黑
2.  基于黑名单的非法广告点击流量过滤机制：
3.  每天各省各城市各广告的点击流量实时统计：
4.  统计每天各省top3热门广告
5.  统计各广告最近1小时内的点击量趋势：各广告最近1小时内各分钟的点击量
6.  使用高性能方式将实时统计结果写入MySQL
7.  实现实时计算程序的HA高可用性（Spark Streaming HA方案）
8.  实现实时计算程序的性能调优（Spark Streaming Performence Tuning方案）

### 技术方案设计
1. 实时计算各batch中的每天各用户对各广告的点击次数
2. 使用高性能方式将每天各用户对各广告的点击次数写入MySQL中（更新）
3. 使用filter过滤出每天对某个广告点击超过100次的黑名单用户，去重后写入MySQL中
4. 使用transform操作对每个batch RDD进行处理，都动态加载MySQL中的黑名单生成RDD，然后进行join后，过滤掉batch RDD中的黑名单用户的广告点击行为
5. 使用updateStateByKey操作，实时计算每天各省各城市各广告的点击量，并时候更新到MySQL
6. 使用transform结合Spark SQL，统计每天各省份top3热门广告：
    1. 首先以每天各省各城市各广告的点击量数据作为基础，统计出每天各省份各广告的点击量；
    2. 然后启动一个异步子线程，使用Spark SQL动态将数据RDD转换为DataFrame后，注册为临时表；
    3. 最后使用Spark SQL开窗函数，统计出各省份top3热门的广告，并更新到MySQL中
7. 使用window操作，对最近1小时滑动窗口内的数据，计算出各广告各分钟的点击量，并更新到MySQL中
8. 实现实时计算程序的HA高可用性
9. 对实时计算程序进行性能调优

### 数据表设计

数据格式介绍：
timestamp	1450702800
province 	Jiangsu	
city 	Nanjing
userid 	100001
adid 	100001

```mysql
CREATE TABLE `ad_user_click_count` (
  `date` varchar(30) DEFAULT NULL,
  `user_id` int(11) DEFAULT NULL,
  `ad_id` int(11) DEFAULT NULL,
  `click_count` int(11) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

CREATE TABLE `ad_blacklist` (
  `user_id` int(11) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

CREATE TABLE `ad_stat` (
  `date` varchar(30) DEFAULT NULL,
  `province` varchar(100) DEFAULT NULL,
  `city` varchar(100) DEFAULT NULL,
  `ad_id` int(11) DEFAULT NULL,
  `click_count` int(11) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

CREATE TABLE `ad_province_top3` (
  `date` varchar(30) DEFAULT NULL,
  `province` varchar(100) DEFAULT NULL,
  `ad_id` int(11) DEFAULT NULL,
  `click_count` int(11) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

CREATE TABLE `ad_click_trend` (
  `date` varchar(30) DEFAULT NULL,
  `ad_id` int(11) DEFAULT NULL,
  `minute` varchar(30) DEFAULT NULL,
  `click_count` int(11) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
```

### 一些问题

#### 实时计算实现HA
如果有些数据丢失，或者节点挂掉；为了不让实时计算程序挂了；必须做一些数据上的冗余副本，保证实时计算程序可以7 * 24小时的运转。
通过一整套方案有3个步骤。

1. updateStateByKey、window等有状态的操作，自动进行checkpoint，必须设置checkpoint目录(容错的文件系统的目录),就自动实现了HA高可用性。
    * SparkStreaming.checkpoint("hdfs://192.168.1.105:9090/checkpoint")
    * checkpoint，相当于是会把数据保留一份在容错的文件系统中，一旦内存中的数据丢失掉；就可以直接从文件系统中读取数据，不需要重新进行计算。

2. 实现RDD高可用性：启动WAL预写日志机制
    * spark streaming，从原理上来说，是通过receiver来进行数据接收的；然后数据会被划分成一个一个的block；block会被组合成一个batch；针对一个batch，会创建一个rdd；启动一个job来执行我们定义的算子操作。
    * receiver接收到数据,立即将数据写入一份到checkpoint目录中磁盘文件中；作为数据的冗余副本。无论你的程序怎么挂掉，或数据丢失，都不会永久性的丢失；因为肯定有副本。
    * spark.streaming.receiver.writeAheadLog.enable true

3. Driver高可用性
    * 第一次在创建和启动StreamingContext的时候，将持续不断地将实时计算程序的元数据写入容错的文件系统，
    * spark-submit脚本中加一些参数；保证在driver挂掉之后，spark集群可以自己将driver重新启动起来；而且driver在启动的时候，不会重新创建一个streaming context，而是从容错文件系统中读取之前的元数据信息，包括job的执行进度，继续接着之前的进度，继续执行。
    * 使用这种机制，就必须使用cluster模式提交，确保driver运行在某个worker上面；
```
JavaStreamingContextFactory contextFactory = new JavaStreamingContextFactory() {
  @Override 
  public JavaStreamingContext create() {
    JavaStreamingContext jssc = new JavaStreamingContext(...);  
    JavaDStream<String> lines = jssc.socketTextStream(...);     
    jssc.checkpoint(checkpointDirectory);                       
    return jssc;
  }
};

JavaStreamingContext context = JavaStreamingContext.getOrCreate(checkpointDirectory, contextFactory);
context.start();
context.awaitTermination();

spark-submit
--deploy-mode cluster
--supervise
```




#### 数据插入mysql
对于咱们这种实时计算程序的mysql插入，有两种pattern（模式）

1、比较挫：每次插入前先查询，看看有没有数据，如果有，则执行insert语句；
如果没有，则执行update语句；好处在于，每个key就对应一条记录；坏处在于，本来对一个分区的数据就是一条insert batch，现在很麻烦，还得先执行select语句，再决定是insert还是update。
j2ee系统，查询某个key的时候，就直接查询指定的key就好。

2、稍微好一点：每次插入记录，你就插入就好，但是需要在mysql库中，给每一个表都加一个时间戳（timestamp），对于同一个key，5秒一个batch，
每隔5秒中就有一个记录插入进去。相当于在mysql中维护了一个key的多个版本。
j2ee系统，查询某个key的时候，还得限定是要order by timestamp desc limit 1，查询最新时间版本的数据
通过mysql来用这种方式，不是很好，很不方便后面j2ee系统的使用
用hbase比较好（timestamp的多个版本，而且它不却分insert和update，统一就是去对某个行键rowkey去做更新）


#### Spark Streaming foreachRDD的正确使用方式

Spark Streaming对DStream对象不能用foreachPartition， 而是使用foreachRDD。

误区一：在driver上创建连接对象（比如网络连接或数据库连接）

如果在driver上创建连接对象，然后在RDD的算子函数内使用连接对象，就需要将连接对象序列化后从driver传递到worker上。
而连接对象（比如Connection对象）通常来说是不支持序列化的，就会报序列化的异常（serialization errors）。
**因此连接对象必须在worker上创建，不要在driver上创建**。

dstream.foreachRDD { rdd =>
  val connection = createNewConnection()  // 在driver上执行
  rdd.foreach { record =>
    connection.send(record) // 在worker上执行
  }
}

误区二：为每一条记录都创建一个连接对象:
```java
dstream.foreachRDD { rdd =>
  rdd.foreach { record =>
    val connection = createNewConnection()
    connection.send(record)
    connection.close()
  }
}
```
通常来说，连接对象的创建和销毁都是很消耗时间的。
因此频繁地创建和销毁连接对象，可能会导致降低spark作业的整体性能和吞吐量。

正确做法一：为每个RDD分区创建一个连接对象
对DStream中的RDD，调用foreachPartition，对RDD中每个分区创建一个连接对象，
使用一个连接对象将一个分区内的数据都写入底层MySQL中。这样可以大大减少创建的连接对象的数量。
```java
dstream.foreachRDD { rdd =>
  rdd.foreachPartition { partitionOfRecords =>
    val connection = createNewConnection()
    partitionOfRecords.foreach(record => connection.send(record))
    connection.close()
  }
}
```

正确做法二：为每个RDD分区使用一个连接池中的连接对象
```java
dstream.foreachRDD { rdd =>
  rdd.foreachPartition { partitionOfRecords =>
    // 静态连接池，同时连接是懒创建的
    val connection = ConnectionPool.getConnection()
    partitionOfRecords.foreach(record => connection.send(record))
    ConnectionPool.returnConnection(connection)  // 用完以后将连接返回给连接池，进行复用
  }
}
```


#### 对于JavaPairInputDStream要用什么算子呢
是mapToPair呢？还是foreachRDD呢？什么区别？？


### 优化

#### 并行化数据接收：处理多个topic的数据时比较有效
```java
int numStreams = 5;
List<JavaPairDStream<String, String>> kafkaStreams = new ArrayList<JavaPairDStream<String, String>>(numStreams);
for (int i = 0; i < numStreams; i++) {
  kafkaStreams.add(KafkaUtils.createStream(...));
}
JavaPairDStream<String, String> unifiedStream = streamingContext.union(kafkaStreams.get(0), kafkaStreams.subList(1, kafkaStreams.size()));
unifiedStream.print();
```

#### spark.streaming.blockInterval：增加block数量，增加每个batch rdd的partition数量，增加处理并行度

receiver从数据源源源不断地获取到数据；首先是会按照block interval，将指定时间间隔的数据，收集为一个block；默认时间是200ms，官方推荐不要小于50ms；接着呢，会将指定batch interval时间间隔内的block，合并为一个batch；创建为一个rdd，然后启动一个job，去处理这个batch rdd中的数据

batch rdd，它的partition数量是多少呢？一个batch有多少个block，就有多少个partition；就意味着并行度是多少；就意味着每个batch rdd有多少个task会并行计算和处理。

当然是希望可以比默认的task数量和并行度再多一些了；可以手动调节block interval；减少block interval；每个batch可以包含更多的block；有更多的partition；也就有更多的task并行处理每个batch rdd。

定死了，初始的rdd过来，直接就是固定的partition数量了

#### inputStream.repartition(<number of partitions>)：重分区，增加每个batch rdd的partition数量

有些时候，希望对某些dstream中的rdd进行定制化的分区
对dstream中的rdd进行重分区，去重分区成指定数量的分区，这样也可以提高指定dstream的rdd的计算并行度

#### 调节并行度
spark.default.parallelism
reduceByKey(numPartitions)

#### 使用Kryo序列化机制：

spark streaming，也是有不少序列化的场景的
提高序列化task发送到executor上执行的性能，如果task很多的时候，task序列化和反序列化的性能开销也比较可观
默认输入数据的存储级别是StorageLevel.MEMORY_AND_DISK_SER_2，receiver接收到数据，默认就会进行持久化操作；首先序列化数据，存储到内存中；如果内存资源不够大，那么就写入磁盘；而且，还会写一份冗余副本到其他executor的block manager中，进行数据冗余。

#### batch interval：每个的处理时间必须小于batch interval
在spark ui上观察spark streaming作业的运行情况的；可以看到batch的处理时间；
如果发现batch的处理时间大于batch interval，就必须调节batch interval；
尽量不要让batch处理时间大于batch interval，不然batch在你的内存中日积月累，一直囤积着，没法及时计算掉，释放内存空间；
而且对内存空间的占用越来越大，那么此时会导致内存空间快速消耗。